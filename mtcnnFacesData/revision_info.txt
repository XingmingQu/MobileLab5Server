arguments: src/align/align_dataset_mtcnn.py ./imageData ./mtcnnFacesData --image_size 160 --margin 32 --random_order
--------------------
tensorflow version: 1.15.0
--------------------
git hash: b'2ab6d242fb56d2b74a65ed6e2b0a564437151b09'
--------------------
b'diff --git a/imageData/Aaron_Eckhart/Aaron_Eckhart_0001.jpg b/imageData/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\ndeleted file mode 100644\nindex 306f298..0000000\nBinary files a/imageData/Aaron_Eckhart/Aaron_Eckhart_0001.jpg and /dev/null differ\ndiff --git a/imageData/Aaron_Guiel/Aaron_Guiel_0001.jpg b/imageData/Aaron_Guiel/Aaron_Guiel_0001.jpg\ndeleted file mode 100644\nindex 00ac2e0..0000000\nBinary files a/imageData/Aaron_Guiel/Aaron_Guiel_0001.jpg and /dev/null differ\ndiff --git a/imageData/Aaron_Patterson/Aaron_Patterson_0001.jpg b/imageData/Aaron_Patterson/Aaron_Patterson_0001.jpg\ndeleted file mode 100644\nindex ac1b4d5..0000000\nBinary files a/imageData/Aaron_Patterson/Aaron_Patterson_0001.jpg and /dev/null differ\ndiff --git a/model/mySVMmodel.pkl b/model/mySVMmodel.pkl\nindex 2505e7b..5a1e918 100644\nBinary files a/model/mySVMmodel.pkl and b/model/mySVMmodel.pkl differ\ndiff --git a/mtcnnFacesData/Aaron_Eckhart/Aaron_Eckhart_0001.png b/mtcnnFacesData/Aaron_Eckhart/Aaron_Eckhart_0001.png\ndeleted file mode 100644\nindex 3b2e991..0000000\nBinary files a/mtcnnFacesData/Aaron_Eckhart/Aaron_Eckhart_0001.png and /dev/null differ\ndiff --git a/mtcnnFacesData/Aaron_Guiel/Aaron_Guiel_0001.png b/mtcnnFacesData/Aaron_Guiel/Aaron_Guiel_0001.png\ndeleted file mode 100644\nindex 5bf2008..0000000\nBinary files a/mtcnnFacesData/Aaron_Guiel/Aaron_Guiel_0001.png and /dev/null differ\ndiff --git a/mtcnnFacesData/Aaron_Patterson/Aaron_Patterson_0001.png b/mtcnnFacesData/Aaron_Patterson/Aaron_Patterson_0001.png\ndeleted file mode 100644\nindex bfd603e..0000000\nBinary files a/mtcnnFacesData/Aaron_Patterson/Aaron_Patterson_0001.png and /dev/null differ\ndiff --git a/mtcnnFacesData/bounding_boxes_05422.txt b/mtcnnFacesData/bounding_boxes_05422.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/mtcnnFacesData/bounding_boxes_25947.txt b/mtcnnFacesData/bounding_boxes_25947.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/mtcnnFacesData/bounding_boxes_52305.txt b/mtcnnFacesData/bounding_boxes_52305.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/mtcnnFacesData/bounding_boxes_93752.txt b/mtcnnFacesData/bounding_boxes_93752.txt\ndeleted file mode 100644\nindex 3dc5727..0000000\n--- a/mtcnnFacesData/bounding_boxes_93752.txt\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-./mtcnnFacesData/Aaron_Guiel/Aaron_Guiel_0001.png 52 49 183 214\n-./mtcnnFacesData/qxm/qxm_4.png 86 101 224 266\n-./mtcnnFacesData/qxm/qxm_2.png 86 101 224 266\n-./mtcnnFacesData/qxm/qxm_3.png 86 101 224 266\n-./mtcnnFacesData/qxm/qxm_5.png 73 112 250 340\n-./mtcnnFacesData/qxm/qxm_6.png 59 96 244 333\n-./mtcnnFacesData/qxm/qxm_1.png 86 101 224 266\n-./mtcnnFacesData/Aaron_Eckhart/Aaron_Eckhart_0001.png 62 50 185 199\n-./mtcnnFacesData/Aaron_Patterson/Aaron_Patterson_0001.png 64 56 185 197\ndiff --git a/mtcnnFacesData/qxm/qxm_1.png b/mtcnnFacesData/qxm/qxm_1.png\ndeleted file mode 100644\nindex 8b8968d..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_1.png and /dev/null differ\ndiff --git a/mtcnnFacesData/qxm/qxm_2.png b/mtcnnFacesData/qxm/qxm_2.png\ndeleted file mode 100644\nindex 8b8968d..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_2.png and /dev/null differ\ndiff --git a/mtcnnFacesData/qxm/qxm_3.png b/mtcnnFacesData/qxm/qxm_3.png\ndeleted file mode 100644\nindex 8b8968d..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_3.png and /dev/null differ\ndiff --git a/mtcnnFacesData/qxm/qxm_4.png b/mtcnnFacesData/qxm/qxm_4.png\ndeleted file mode 100644\nindex 8b8968d..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_4.png and /dev/null differ\ndiff --git a/mtcnnFacesData/qxm/qxm_5.png b/mtcnnFacesData/qxm/qxm_5.png\ndeleted file mode 100644\nindex 6256f79..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_5.png and /dev/null differ\ndiff --git a/mtcnnFacesData/qxm/qxm_6.png b/mtcnnFacesData/qxm/qxm_6.png\ndeleted file mode 100644\nindex 730358b..0000000\nBinary files a/mtcnnFacesData/qxm/qxm_6.png and /dev/null differ\ndiff --git a/mtcnnFacesData/revision_info.txt b/mtcnnFacesData/revision_info.txt\ndeleted file mode 100644\nindex 856f336..0000000\n--- a/mtcnnFacesData/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: src/align/align_dataset_mtcnn.py ./imageData ./mtcnnFacesData --image_size 160 --margin 32 --random_order\n---------------------\n-tensorflow version: 1.15.0\n---------------------\n-git hash: b\'5f8cfdb724f42eb90292da038f088d669a60f6d7\'\n---------------------\n-b\'diff --git a/sklearnhandlers.py b/sklearnhandlers.py\\nindex afea865..a845e45 100644\\n--- a/sklearnhandlers.py\\n+++ b/sklearnhandlers.py\\n@@ -9,7 +9,7 @@ from tornado.ioloop import IOLoop\\n from tornado.options import define, options\\n \\n from basehandler import BaseHandler\\n-\\n+import subprocess as sp\\n from sklearn.neighbors import KNeighborsClassifier\\n import pickle\\n from bson.binary import Binary\\n@@ -97,49 +97,109 @@ class UpdateModelForDatasetId(BaseHandler):\\n         \\\'\\\'\\\'\\n         dsid = self.get_int_arg("dsid",default=0)\\n \\n-        # create feature vectors from database\\n-        f=[];\\n-        for a in self.db.labeledinstances.find({"dsid":dsid}): \\n-            f.append([float(val) for val in a[\\\'feature\\\']])\\n-\\n-        # create label vector from database\\n-        l=[];\\n-        for a in self.db.labeledinstances.find({"dsid":dsid}): \\n-            l.append(a[\\\'label\\\'])\\n-\\n-        # fit the model to the data\\n-        c1 = KNeighborsClassifier(n_neighbors=1);\\n-        acc = -1;\\n-        if l:\\n-            c1.fit(f,l) # training\\n-            lstar = c1.predict(f)\\n-            self.clf = c1\\n-            acc = sum(lstar==l)/float(len(l))\\n-            bytes = pickle.dumps(c1)\\n+        # # create feature vectors from database\\n+        # f=[];\\n+        # for a in self.db.labeledinstances.find({"dsid":dsid}): \\n+        #     f.append([float(val) for val in a[\\\'feature\\\']])\\n+\\n+        # # create label vector from database\\n+        # l=[];\\n+        # for a in self.db.labeledinstances.find({"dsid":dsid}): \\n+        #     l.append(a[\\\'label\\\'])\\n+\\n+        # # fit the model to the data\\n+        # c1 = KNeighborsClassifier(n_neighbors=1);\\n+\\n+        data_folder = \\\'./mtcnnFacesData\\\'\\n+        face_net_model = \\\'/Users/xqu/datasets/pretrain/20180402-114759.pb\\\'\\n+        output_model = \\\'./model/mySVMmodel.pkl\\\'\\n+        batch_size = 10\\n+        augment_times = 20\\n+\\n+        face_detection_corp_face ="""python src/align/align_dataset_mtcnn.py \\\\\\n+                                ./imageData \\\\\\n+                                ./mtcnnFacesData \\\\\\n+                                --image_size 160 \\\\\\n+                                --margin 32 \\\\\\n+                                --random_order\\n+        """\\n+\\n+        print("Now croping face from image........\\\\n\\\\n")\\n+        flag=sp.call(face_detection_corp_face,shell=True)\\n+        if flag!=0:\\n+            raise Exception(\\\'Please check python src/align/align_dataset_mtcnn.py  cmd \\\')\\n+        else:\\n+            print(\\\'\\\\nFinished\\\')\\n+\\n+        cmd = """python myclassifier.py TRAIN \\\\\\n+        {} \\\\\\n+        {} \\\\\\n+        {} \\\\\\n+        --batch_size {} \\\\\\n+        --augment_times {}""".format(data_folder,face_net_model,output_model,batch_size,augment_times)\\n+\\n+        print("Now runing myclassifer........")\\n+        flag=sp.call(cmd,shell=True)\\n+        if flag!=0:\\n+            raise Exception(\\\'Please check python myclassifier.py cmd \\\')\\n+        else:\\n+            print(\\\'\\\\nFinished\\\')\\n+\\n+        with open(output_model, \\\'rb\\\') as infile:\\n+            (model, class_names) = pickle.load(infile)\\n+\\n+\\n+            self.clf = model\\n+            bytes = pickle.dumps(self.clf)\\n             self.db.models.update({"dsid":dsid},\\n                 {  "$set": {"model":Binary(bytes)}  },\\n                 upsert=True)\\n \\n-        # send back the resubstitution accuracy\\n-        # if training takes a while, we are blocking tornado!! No!!\\n-        self.write_json({"resubAccuracy":acc})\\n+            # send back the resubstitution accuracy\\n+            # if training takes a while, we are blocking tornado!! No!!\\n+            self.write_json({"log":"Finished model training"})\\n \\n class PredictOneFromDatasetId(BaseHandler):\\n     def post(self):\\n         \\\'\\\'\\\'Predict the class of a sent feature vector\\n         \\\'\\\'\\\'\\n-        data = json.loads(self.request.body.decode("utf-8"))    \\n-\\n-        vals = data[\\\'feature\\\'];\\n-        fvals = [float(val) for val in vals];\\n-        fvals = np.array(fvals).reshape(1, -1)\\n-        dsid  = data[\\\'dsid\\\']\\n-\\n-        # load the model from the database (using pickle)\\n-        # we are blocking tornado!! no!!\\n-        if(self.clf == []):\\n-            print(\\\'Loading Model From DB\\\')\\n-            tmp = self.db.models.find_one({"dsid":dsid})\\n-            self.clf = pickle.loads(tmp[\\\'model\\\'])\\n-        predLabel = self.clf.predict(fvals);\\n-        self.write_json({"prediction":str(predLabel)})\\n+        # data = json.loads(self.request.body.decode("utf-8"))    \\n+\\n+        # vals = data[\\\'feature\\\'];\\n+        # fvals = [float(val) for val in vals];\\n+        # fvals = np.array(fvals).reshape(1, -1)\\n+        # dsid  = data[\\\'dsid\\\']\\n+\\n+        # # load the model from the database (using pickle)\\n+        # # we are blocking tornado!! no!!\\n+        # if(self.clf == []):\\n+        #     print(\\\'Loading Model From DB\\\')\\n+        #     tmp = self.db.models.find_one({"dsid":dsid})\\n+        #     self.clf = pickle.loads(tmp[\\\'model\\\'])\\n+        # predLabel = self.clf.predict(fvals);\\n+        # self.write_json({"prediction":str(predLabel)})\\n+        data = json.loads(self.request.body.decode("utf-8"))\\n+\\n+        vals = data[\\\'feature\\\']\\n+        image = stringToRGB(vals)\\n+        # vals = image\\n+        print(\\\'\\\\n\\\\n\\\\n\\\\n\\\',image.shape)\\n+        # fvals = [float(val) for val in vals]\\n+        label = data[\\\'label\\\']\\n+        sess  = data[\\\'dsid\\\']\\n+\\n+        face = self.faceEmbedding(image)\\n+\\n+        print(\\\'\\\\n\\\\n\\\\n\\\',face.shape)\\n+        predictions = self.clf.predict_proba(emb_array)\\n+        best_class_indices = np.argmax(predictions, axis=1)\\n+        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\n+        if best_class_probabilities[0] > 0.3:\\n+            pre=float(best_class_probabilities[i])*100\\n+            pre= round(pre,2)\\n+            pre=str(pre)+\\\'%\\\'\\n+            a=class_names[best_class_indices[i]].split(\\\' \\\')\\n+            result=a[0]\\n+            self.write_json({"prediction":str(result+\\\' \\\'+pre)})\\n+        else:\\n+            self.write_json({"prediction":str("UNKNOWN")})\\ndiff --git a/tornado_scikit_learn.py b/tornado_scikit_learn.py\\nindex c32b2db..50f717b 100644\\n--- a/tornado_scikit_learn.py\\n+++ b/tornado_scikit_learn.py\\n@@ -79,7 +79,7 @@ class Application(tornado.web.Application):\\n         self.handlers_string = str(handlers)\\n \\n         try:\\n-            self.client  = MongoClient(serverSelectionTimeoutMS=50) # local host, default port\\n+            self.client  = MongoClient(serverSelectionTimeoutMS=999999) # local host, default port\\n             print(self.client.server_info()) # force pymongo to look for possible running servers, error if none running\\n             # if we get here, at least one instance of pymongo is running\\n             self.db = self.client.sklearndatabase # database with labeledinstances, models\'\n\\ No newline at end of file\ndiff --git a/mypredictor.py b/mypredictor.py\nindex a6365ee..f5520b2 100644\n--- a/mypredictor.py\n+++ b/mypredictor.py\n@@ -149,7 +149,7 @@ def main():\n \n \t                    best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n \t                    print (best_class_probabilities)\n-\t                    if best_class_probabilities[0] > 0.3 and right < best_class_probabilities[0]:\n+\t                    if best_class_probabilities[0] > 0.5 and right < best_class_probabilities[0]:\n \n \t\t                    pre=float(best_class_probabilities[i])*100\n \t\t                    pre= round(pre,2)\ndiff --git a/sklearnhandlers.py b/sklearnhandlers.py\nindex 5f22993..9f9d38c 100644\n--- a/sklearnhandlers.py\n+++ b/sklearnhandlers.py\n@@ -201,7 +201,17 @@ class PredictOneFromDatasetId(BaseHandler):\n         best_class_indices = np.argmax(predictions, axis=1)\n         best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n         \n-        if best_class_probabilities[0] > 0.3:\n+        sig_test = list(predictions[0])\n+        del sig_test[best_class_indices[0]]\n+\n+        arrstd = np.std(sig_test)\n+        arrmean = np.mean(sig_test)\n+        right = arrmean+3.5*arrstd\n+        print(sig_test)\n+        print(arrmean,arrstd,right,best_class_probabilities[0])\n+\n+\n+        if best_class_probabilities[0] > 0.375 and best_class_probabilities[0]> right:\n             pre=float(best_class_probabilities[0])*100\n             pre= round(pre,2)\n             pre=str(pre)+\'%\''